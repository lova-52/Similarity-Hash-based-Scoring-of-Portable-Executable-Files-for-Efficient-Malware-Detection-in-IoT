import pandas as pd
import ssdeep

from tabulate import tabulate
from utils.hash_similarity import *
from utils.data_loader import *
from utils.tlbsa import *

def main():
    # Load datasets
    dataset_I = load_dataset('test1.csv')
    dataset_II = load_dataset('test2.csv')

    #Get total samples of both datasets
    total_sameples_I = len(dataset_I)
    total_sameples_II = len(dataset_II)

    # Calculate detection rates
    print("Calculating det rates: ")
    det_rates = calculate_det_rates(dataset_I, dataset_II)

    # Calculate CFI value for the hash methods
    print("Calculating CFI value: ")
    cfi = calculate_cfi(det_rates)
    print(cfi)

    # Compute belief factors for each hash method
    print("Computing belief factors for each hash method:")
    esf = compute_esf(det_rates, cfi, dataset_I, dataset_II)
    print(esf)

    # Calculate Recall value for the hash methods
    recall = calculate_recall(det_rates)

    # Calculate PPV value for the hash methods
    ppv = calculate_ppv(det_rates)

    # Calculate ACC value for the hash methods
    acc = calculate_acc(det_rates)

    # Calculate F1 value for the hash methods
    f1 = calculate_f1(det_rates)

    # Calculate TDR value for the hash methods
    tdr = calculate_tdr(det_rates, total_sameples_II)

    # Calculate FDR value for the hash methods
    fdr = calculate_fdr(det_rates, total_sameples_II)
    

    # Format results as table
    headers = ["Method", "Recall", "PPV", "ACC", "F1", "TDR", "FDR", "CFI"]
    table = []
    for method in det_rates.keys():
        row = [
            method,
            recall[method],
            ppv[method],
            acc[method],
            f1[method],
            tdr[method],
            fdr[method],
            cfi[method]
        ]
        table.append(row)
    print(tabulate(table, headers=headers))

if __name__ == "__main__":
    main()
